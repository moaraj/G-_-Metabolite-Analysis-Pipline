---
title: "Tree.Classificaiton"
output: html_document
---

```{r setup, include=FALSE}
library(rpart.plot)
library(rpart)
library(plotmo)
library(lattice)
library(rattle)
```

```{r}
tree.data <- metab_data4[,c(8,12:ncol(metab_data4))]
tree.data <- tree.data %>% filter(Diet %in% c("CD","HF")) %>% select(-one_of("Allyl.isothiocyanate"))
f.rp  <- rpart(Diet ~ ., data=tree.data)
prp(f.rp, type = 3, extra = 3)


## slightly larger than default:
(f.rp2 <- rpart(Diet ~ ., data=tree.data, minsplit = 8, cp = .1))
prp(f.rp2, type=1, extra=3, box.col = f.rp2$y)

(f.rp3 <- rpart(Diet ~ ., data=tree.data, minsplit = 8, cp = .01))
prp(f.rp3, type=1, extra=3,box.col = f.rp3$y)

(f.rp4 <- rpart(Diet ~ ., data=tree.data, minsplit = 8, cp = .001))
prp(f.rp4, type=1, extra=3,box.col = f.rp4$y)

(f.rp5 <- rpart(Diet ~ ., data=tree.data, minsplit = 8, cp = .0001))
prp(f.rp5, type=1, extra=3)

fancyRpartPlot(f.rp2)
fancyRpartPlot(f.rp3)
fancyRpartPlot(f.rp4)
fancyRpartPlot(f.rp5)

## For Cp-plot, we want a more "interesting" (larger) example
plotcp(f.rp2) 
plotcp(f.rp3) 
plotcp(f.rp4)


# silly too large model
f.rp.age <- rpart(Diet ~ ., data = tree.data,
               control = rpart.control(minsplit = 5, cp=0))

tree.data <- metab_data4[,c(8,11,12:ncol(metab_data4))]
tree.data <- filter(tree.data, Age.Cohort %in% c(1:4))  %>% filter(Diet == "CD")
f.rp.age.0  <- rpart(Age.Cohort ~ ., data=tree.data)

prp(f.rp)

## slightly larger than default:
(f.rp2.age.1 <- rpart(Age.Cohort ~ ., data=tree.data, minsplit = 4, cp = .1))
prp(f.rp2.age.1, type=3, extra=3)

(f.rp2.age.2 <- rpart(Age.Cohort ~ ., data=tree.data, minsplit = 4, cp = .01))
prp(f.rp2.age.2, type=3, extra=3)

(f.rp2.age.3 <- rpart(Age.Cohort ~ ., data=tree.data, minsplit = 4, cp = .001))
prp(f.rp2.age.3, type=3, extra=3)

## For Cp-plot, we want a more "interesting" (larger) example
plotcp(f.rp) #
plotcp(f.rp2) 
```

# PARTY package
```{r}
library(party)

frmla = Diet ~ .
 
(ct = ctree(frmla, data = tree.data))
plot(ct, main="Conditional Inference Tree")
 
#Table of prediction errors
table(predict(ct), raw$Metal)
 
# Estimated class probabilities
tr.pred = predict(ct, newdata=raw, type="prob")
```

# MAPTREE
```{r}
library(maptree)
library(cluster)
draw.tree( clip.rpart (rpart ( tree.data), best=7),
nodeinfo=TRUE, units="species",
cases="cells", digits=0)
a = agnes ( raw[2:4], method="ward" )
names(a)
a$diss
b = kgs (a, a$diss, maxclust=20)
 
plot(names(b), b, xlab="# clusters", ylab="penalty", type="n")
xloc = names(b)[b==min(b)]
yloc = min(b)
ngon(c(xloc,yloc+.75,10, "dark green"), angle=180, n=3)
apply(cbind(names(b), b, 3, 'blue'), 1, ngon, 4) # cbind(x,y,size,color)
```


## EVTREE (Evoluationary Learning)
```{r}
library(evtree)
 
ev.raw = evtree(frmla, data=tree.data)
plot(ev.raw)
table(predict(ev.raw), raw$Metal)
1-mean(predict(ev.raw) == raw$Metal)
```


## randomForest
```{r}
library(randomForest)
tree.data[1] <- droplevels(tree.data[1])

fit.rf = randomForest(data=tree.data, Diet ~ . )
print(fit.rf)
importance(fit.rf)
plot(fit.rf)
plot( importance(fit.rf), lty=2, pch=16)
lines(importance(fit.rf))
imp = importance(fit.rf)
impvar = rownames(imp)[order(imp[, 1], decreasing=TRUE)]
op = par(mfrow=c(1, 3))

for (i in seq_along(impvar)) {
partialPlot(fit.rf, raw, impvar[i], xlab=impvar[i],
main=paste("Partial Dependence on", impvar[i]),
ylim=c(0, 1))
}

```


## CORElearn
```{r} 
library(CORElearn)
## Random Forests
fit.rand.forest = CoreModel(frmla, data=tree.data, model="rf", selectionEstimator="MDL", minNodeWeightRF=5, rfNoTrees=100)
plot(fit.rand.forest)
 
## decision tree with naive Bayes in the leaves
fit.dt = CoreModel(frmla, tree.data, model="tree", modelType=4)
plot(fit.dt, tree.data)
 
fit.rt = CoreModel(Diet~., tree.data, model="regTree", modelTypeReg=1)
summary(fit.rt)

plot(fit.rt, tree.data, graphType="prototypes")
 
pred = predict(fit.dt, tree.data)
print(pred)
plot(pred$class)
```